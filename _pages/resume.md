---
name: "Resume"
permalink: /resume/
layout: single
print_style: true
---

# Suyeong Kim 
<p style="text-align: center;"> sy.from.kr@gmail.com | hello-iam-sy.github.io | Sydney, NSW</p>

## CAREER PROFILE
Experienced **Data Engineer** with **5 years of experience** in **data platform engineering and data pipeline architecture** in South Korea. My expertise lies in **building and optimizing data infrastructure**, ensuring **efficient data processing, accessibility, and reliability** to support analytical and operational needs.

Currently pursuing a **Master of Computer Science** at **The University of Sydney**, focusing on **Data Science and Artificial Intelligence**, to enhance my ability to develop **data-driven decision-making systems**. I am particularly interested in **leveraging AI and advanced analytics** to drive **operational efficiency and digital transformation** within organizations.

With my strong background in **data platform development, automation, and optimization**, I aim to transition into the **consulting and digital transformation space**, where I can contribute by **enhancing data-driven strategies, streamlining business processes, and implementing AI-powered solutions**. I am eager to bring my expertise to **Visagio**, applying **innovative data engineering solutions** to support **business transformation, operational efficiency, and AI-driven decision-making**. 

## EDUCATION    
<div style="display: flex; justify-content: space-between;">
  <span>
    <span>Master of Computer Science</span><br>
    <span>The University of Sydney</span>
  </span>
  <span style="font-size: 12px;">February 2025 - expected Dec 2026</span>
</div>

<div style="display: flex; justify-content: space-between; margin-bottom: 20px;">
  <span>
    <span>Bachelor in Mechanical and System Design Engineering</span><br>
    <span>Hongik University</span>
  </span>
  <span style="font-size: 12px;">March 2014 - August 2019</span>
</div>

## RELEVANT EXPERIENCE  

<div style="display: flex; justify-content: space-between; margin-bottom: 10px; font-size: 18px;">
  <span>
    <strong>LGCNS</strong><br>
  </span>
  <span style="font-size: 12px;">Seoul, Republic of Korea</span>
</div>

<div style="display: flex; justify-content: space-between; margin-bottom: 20px; font-size: 16px;">
  <span>
    <strong>Data Platform Architecture Team / Data Engineer</strong><br>
  </span>
  <span style="font-size: 12px;">July 2019 - October 2024</span>
</div>

### Implementation of New Big Data Platform Components
- Designed and integrated **metadata management and data lineage tracking systems** using **DataHub** to enhance transparency and traceability.
- Modified **container-based deployment** (originally Docker/Kubernetes) to function in an **on-premise environment**, optimizing **metadata management**.
- Connected **S3, Hive, Delta Lake, and PostgreSQL** to DataHub, improving **data discoverability and quality control**.
- Developed concise **API documentation and internal usage guides**, ensuring smooth adoption by data teams.
- **Technologies used**: Python, DataHub, Hadoop, Ambari, Docker, Shell  

### Data API Testing and Validation
- Developed and tested **API-driven data pipelines** to ensure secure, high-speed access to large-scale datasets.
- Simulated **AWS-like API behavior** for an **on-premise data platform**, integrating services like **HDFS, S3, GreenPlum DB, and DynamoDB**.
- **Technologies used**: Python, Pandas, HDFS, S3, GreenPlum DB, Hive, Athena  

### Home Appliance Demand Forecasting Service
- Maintained and optimized **forecasting models for demand prediction**, ensuring accurate weekly updates.
- Collaborated with data scientists to **enhance machine learning models** by integrating **new data features**.
- Built an **automated Slack-based alerting system** for real-time monitoring of **ETL pipelines**.
- **Technologies used**: Informatica, BigQuery, Python, Pandas  

## TECHNICAL SKILLS  
**Programming Languages**: Python, SQL, Shell  
**Data Engineering**: Hadoop, Kafka, Spark, ETL Pipelines, API Development  
**Cloud Platforms**: AWS, GCP  
**Machine Learning & AI**: Scikit-learn, TensorFlow (beginner level)  
**Languages**: Korean (Native), English (Proficient)  