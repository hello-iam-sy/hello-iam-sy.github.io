---
name: "Resume"
permalink: /resume/
layout: single
print_style: true
---

# Suyeong Kim 
<p style="text-align: center;"> sy.from.kr@gmail.com | hello-iam-sy.github.io | Sydney, NSW</p>


## CAREER PROFILE
Experienced in data engineering, data platform architecture, project management, and system maintenance, with a strong foundation in building and managing data solutions. My career in South Korea has been focused on enabling businesses to derive value from data by designing and maintaining data platforms.

Currently pursuing a Master’s in Software Engineering in Australia to develop expertise in Data Science and AI. While I am still at the beginning of my journey with AI and ML algorithms, my goal is to apply these technologies in real-world operations by building effective DevOps and MLOps pipelines. I am passionate about leveraging this expertise to create scalable systems that integrate advanced analytics into practical, everyday applications.


## EDUCATION    
<!-- **Master of Professional Engineering (Software Engineering)**  | February 2025 - expected Dec 2026    -->
<div style="display: flex; justify-content: space-between;">
  <span>
    <span>Master of Professional Engineering (Software Engineering)</span><br>
    <span>The University of Sydney</span>
  </span>
  <span style="font-size: 12px;">February 2025 - expected Dec 2026</span>
</div>

<div style="display: flex; justify-content: space-between; margin-bottom: 20px;">
  <span>
    <span>Bachelor in Mechanical and System Design Engineering</span><br>
    <span>Hongik University</span>
  </span>
  <span style="font-size: 12px;">March 2014 - August 2019</span>
</div>


## RELEVANT EXPERIENCE  

<div style="display: flex; justify-content: space-between; margin-bottom: 10px; font-size: 18px;">
  <span>
    <strong>LGCNS</strong><br>
  </span>
  <span style="font-size: 12px;">Seoul, Republic of Korea</span>
</div>

<div style="display: flex; justify-content: space-between; margin-bottom: 20px; font-size: 16px;">
  <span>
    <strong>Data Platform Architecture Team / Data Engineer</strong><br>
  </span>
  <span style="font-size: 12px;">July 2019 - October 2024</span>
</div>

<div style="display: flex; justify-content: space-between; margin-bottom: 10px; font-size: 16px;">
  <span>
    <strong>Implementation of New Big Data Platform Components</strong><br>
  </span>
  <span style="font-size: 12px;">February 2024 - October 2024</span>
</div>

- Conducted a technical assessment of DataHub, an open-source metadata management platform, and provided detailed technical recommendations to stakeholders, justifying its adoption for metadata management, data lineage, and quality control within the existing Hadoop-based on-premise big data environment.  
- Adapted the deployment process of DataHub, originally designed for Docker/Kubernetes, to fit the on-premise Hadoop ecosystem by running it as a customized process for seamless integration.  
- Configured and integrated DataHub with organizational tools, including S3, Hive, Delta Lake, and PostgreSQL, enabling robust metadata tracking.  
- Documented the key APIs from DataHub’s documentation, tailored to the team’s needs, and created a concise usage guide to ensure effective adoption and consistent use of the platform.   
- Collaborated with cross-functional teams to ensure successful deployment and alignment with organizational workflows.  
- **Technologies used**: Python, DataHub, Docker, Shell, Ambari  

<div style="display: flex; justify-content: space-between; margin-bottom: 10px; font-size: 16px;">
  <span>
    <strong>Automation of User Sandbox Creation</strong><br>
  </span>
  <span style="font-size: 12px;">June 2023 - October 2023</span>
</div>

- Built OS image templates for CentOS, Rocky Linux, and Ubuntu to support user sandboxes, ensuring seamless GPU-enabled data analysis in an on-premise environment.  
- Configured Nvidia A40 and A100 GPUs by installing GPU drivers, CUDA, and PyTorch, ensuring compatibility with various hardware setups.  
- Resolved installation and configuration issues related to GPU drivers and software dependencies, enabling seamless GPU utilization.  
- **Technologies used**: OS, vRealize Automation  

<div style="page-break-after: always;"></div>


<div style="display: flex; justify-content: space-between; margin-bottom: 10px; font-size: 16px;">
  <span>
    <strong>Data API Testing and Validation</strong><br>
  </span>
  <span style="font-size: 12px;">March 2022 - May 2023</span>
</div>


- Validated the functionality of APIs to ensure seamless operation for data security, access control, and efficient data access within the organization.   
- Simulated AWS-like behavior by configuring and testing the boto3 client in an on-premise environment to ensure compatibility with existing infrastructure.  
- Designed and executed comprehensive API test cases, verifying performance and reliability before deployment.   
- **Technologies used**: HDFS, S3, GreenPlum DB, DynamoDB, Hive, Athena, Python, Pandas  

<div style="display: flex; justify-content: space-between; margin-bottom: 10px; font-size: 16px;">
  <span>
    <strong>Home Appliance Demand Forecasting Service</strong><br>
  </span>
  <span style="font-size: 12px;">February 2020 - March 2021</span>
</div>

- Operated and maintained the weekly demand forecasting pipeline for home appliance products, ensuring accurate and timely predictions.    
- Managed and monitored ETL processes using Informatica, addressing data discrepancies and ensuring pipeline reliability.  
- Collaborated with data scientists to incorporate requested features into the data mart, enhancing the forecasting model's performance.  
- Validated and resolved data integrity issues using BigQuery and Pandas for detailed analysis and troubleshooting.  
- Developed and implemented a Slack-based alert system to proactively monitor pipeline performance and address anomalies.    
- **Technologies used**: Informatica, BigQuery, Python, Pandas  


## TECHNICAL SKILLS  
**Programming Languages**: Python, SQL, Shell  
**Data Engineering**: Hadoop, ETL  
**Cloud Platforms**: AWS, GCP  
**Languages**: Korean(native), English(Proficient)  